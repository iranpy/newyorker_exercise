{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import lightning.pytorch as pl\n",
    "from tqdm.autonotebook import tqdm\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import  TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# Monkey path for backward compatibility\n",
    "np.float = float    \n",
    "np.int = int   #module 'numpy' has no attribute 'int'\n",
    "np.object = object    #module 'numpy' has no attribute 'object'\n",
    "np.bool = bool    #module 'numpy' has no attribute 'bool\n",
    "\n",
    "def read_df(base_path):\n",
    "    df = pd.read_csv(base_path)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "df = read_df('{}/sales.csv'.format(os.path.dirname(os.getcwd())))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2-1: What can be said about the overall trend and seasonality of sales? What of the\n",
    "# individual product type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_sales(data):\n",
    "    # Calculate the overall sales trend\n",
    "    dates = pd.to_datetime(data['date'])\n",
    "\n",
    "    sales = data.iloc[:, 1:].sum(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dates, sales, marker='o')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Overall Sales Trend')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.show()\n",
    "    \n",
    "    # # Calculate and plot the seasonality of individual product types\n",
    "    product_types = data.columns[1:]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # dates2 = df.story_point.resample('W', on='date').sum()\n",
    "    for product_type in product_types:\n",
    "        plt.plot(dates, data[product_type], label=product_type)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Individual Product Sales')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function to analyze the sales\n",
    "analyze_sales(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def trend_season_decompose(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    observed_dict = {}\n",
    "\n",
    "    trend_dict = {}\n",
    "    seasonal_dict = {}\n",
    "\n",
    "    for ts in df.columns:\n",
    "        decomposition = seasonal_decompose(df[ts].dropna(),model='multiplicative',period=4)\n",
    "        observed_dict[ts] = decomposition.observed\n",
    "        trend_dict[ts] = decomposition.trend\n",
    "        seasonal_dict[ts] = decomposition.seasonal\n",
    "\n",
    "    pd.DataFrame(observed_dict).plot(figsize=(20,10),subplots=True, layout=(4, 3), linewidth=1);\n",
    "    pd.DataFrame(trend_dict).plot(figsize=(20,10), subplots=True, layout=(4, 3), linewidth=1);\n",
    "    pd.DataFrame(seasonal_dict).plot(figsize=(20,10), subplots=True, layout=(4, 3), linewidth=1);\n",
    "trend_season_decompose(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2-2: Are there correlations between sales of some product types, and if so, which?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_correlation(df):\n",
    "\n",
    "    df_num = df[df.columns[1:]]\n",
    "\n",
    "    cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\"light\", as_cmap=True)\n",
    "    matrix = df_num.corr(method=\"pearson\")\n",
    "    # Create a mask\n",
    "    mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    sns.heatmap(matrix, mask=mask, cmap=cmap, square=True, annot=True, fmt=\".2f\", ax=ax)\n",
    "    plt.show();\n",
    "timeseries_correlation(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### seasonality of each time series influences others:\n",
    "\n",
    "def seasonality_influences(df):\n",
    "    '''\n",
    "    seasonality of each time series influences others\n",
    "    '''\n",
    "    df_num = df[df.columns[1:]]\n",
    "    seasonality_dict = {\n",
    "        ts: seasonal_decompose(df[ts], period=4).seasonal for ts in df_num.columns\n",
    "    }\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    seasonality_corr = pd.DataFrame(seasonality_dict).corr()\n",
    "    sns.clustermap(seasonality_corr, annot=True, figsize=(8,6))\n",
    "    plt.show();\n",
    "\n",
    "seasonality_influences(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### trends of each time series influences others:\n",
    "def trend_influences(df):\n",
    "    df_num = df[df.columns[1:]]\n",
    "    trend_dict = {\n",
    "        ts: seasonal_decompose(df_num[ts].dropna(), period=4).trend for ts in df_num.columns\n",
    "    }\n",
    "\n",
    "    # Compute corr matrix\n",
    "    trend_corr = pd.DataFrame(trend_dict).corr()\n",
    "\n",
    "    sns.clustermap(trend_corr, annot=True,figsize=(8,6))\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "    normalized = df_num.div(df_num.iloc[0]).mul(100)\n",
    "\n",
    "    normalized.plot(figsize=(16, 8), title=\"Growth of Production in Different sales Sectors\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Increase (%)\");\n",
    "\n",
    "trend_influences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2-3 Select a single product type and make forecast about its sales for 5 time periods\n",
    "# (weeks) from the last observed data point. Please include confidence interval of this\n",
    "# forecast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_preprocessing(df):\n",
    "    data = df[[\"Dress\",\"Blouse\",\"Hoodie\",\"Jacket\",\"Shorts\",\"Skirt\",\"T-shirt\"]]\n",
    "    data['date'] = pd.to_datetime(df['date'])\n",
    "    data['group_id'] = \"1\"\n",
    "    data[\"time_idx\"] = range(0, len(data))\n",
    "    data[\"month\"] = data.date.dt.month.astype(str)  # categories have be strings\n",
    "    return data\n",
    "feature_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 5 # For the next 5 weeks prediction\n",
    "max_encoder_length = 12\n",
    "def prepair_dataset(df):\n",
    "    data = feature_preprocessing(df)\n",
    "    training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx <= training_cutoff],\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"Dress\",\n",
    "        group_ids=[\"group_id\"],\n",
    "        min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[\"group_id\"],\n",
    "        time_varying_known_categoricals=[\"month\"],\n",
    "        time_varying_unknown_reals=[\n",
    "            \"Dress\",\n",
    "            \"Blouse\",\n",
    "            \"Hoodie\",\n",
    "            \"Jacket\",\n",
    "            \"Shorts\",\n",
    "            \"Skirt\",\n",
    "            \"T-shirt\"\n",
    "        ],\n",
    "        target_normalizer=GroupNormalizer(\n",
    "            groups=[\"group_id\"], transformation=\"softplus\"\n",
    "        ),  # use softplus and normalize by group\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "    batch_size = 8\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=5)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=5)\n",
    "\n",
    "    return training, train_dataloader, val_dataloader\n",
    "prepair_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_tuner(df):\n",
    "    training, train_dataloader, val_dataloader = prepair_dataset(df)\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    # logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "    pl.seed_everything(42)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "        # of the gradient for recurrent neural networks\n",
    "        gradient_clip_val=0.1,\n",
    "        log_every_n_steps=12,\n",
    "        callbacks=[lr_logger, early_stop_callback])\n",
    "    # ,\n",
    "        # logger=logger)\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.03,\n",
    "        hidden_size=8,  # most important hyperparameter apart from learning rate\n",
    "        # number of attention heads. Set to up to 4 for large datasets\n",
    "        attention_head_size=1,\n",
    "        dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "        hidden_continuous_size=4,  # set to <= hidden_size\n",
    "        output_size=7,  # 7 quantiles by default\n",
    "        loss=QuantileLoss(),\n",
    "        # reduce learning rate if no improvement in validation loss after x epochs\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    tuner = pl.tuner.Tuner(trainer)\n",
    "    res = tuner.lr_find(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "        max_lr=10.0,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "\n",
    "    print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "    fig = res.plot(show=True, suggest=True)\n",
    "    fig.show()\n",
    "    trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader)\n",
    "    \n",
    "    return trainer, train_dataloader, val_dataloader\n",
    "trainer, train_dataloader, val_dataloader = model_tuner(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def result_explainer(trainer,val_dataloader):\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "    predictions = best_tft.predict(val_dataloader)\n",
    "    (actuals - predictions).abs().mean()\n",
    "\n",
    "\n",
    "    raw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "    for idx in range(1): # nb of groups combinations\n",
    "        fig, ax = plt.subplots(figsize=(15,5))\n",
    "        best_tft.plot_prediction(raw_predictions.x, # network input\n",
    "                                raw_predictions.output, # network output\n",
    "                                idx=idx,\n",
    "                                add_loss_to_title=True,\n",
    "                                ax=ax)\n",
    "result_explainer(trainer, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
